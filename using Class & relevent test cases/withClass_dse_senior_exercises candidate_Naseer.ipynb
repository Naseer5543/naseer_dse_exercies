{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this exercise we expect you to demonstrate your ability to / knowledge of:\n",
    "- optimization\n",
    "- error debugging\n",
    "- Performance improvement\n",
    "- OOPS\n",
    "- Git Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the efficiency of following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704982704\n",
      "for loop method took 0.015994548797607422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N-Workstation\\AppData\\Local\\Temp\\ipykernel_449976\\1055407611.py:4: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total = i + total\n"
     ]
    }
   ],
   "source": [
    "starttime1=time.time()\n",
    "total = 0\n",
    "for i in np.arange(100000):\n",
    "    total = i + total\n",
    "endtime1=time.time()\n",
    "f_time=endtime1-starttime1\n",
    "print(total)\n",
    "print(f'for loop method took {f_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for loop method took 0.0\n",
      "704982704\n"
     ]
    }
   ],
   "source": [
    "#Np.sum can handle in optimized manner\n",
    "\n",
    "starttime1=time.time()\n",
    "\n",
    "total_v2 = 0\n",
    "n = 100000\n",
    "total_v2 = np.sum(np.arange(n))\n",
    "endtime1=time.time()\n",
    "\n",
    "f_time=endtime1-starttime1\n",
    "print(f'for loop method took {f_time}')\n",
    "print(total_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idenitify code issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "url=\"https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv\"\n",
    "s=requests.get(url).content\n",
    "c=pd.read_csv(io.StringIO(s.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N-Workstation\\AppData\\Local\\Temp\\ipykernel_449976\\3670693239.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  c[c.Country=='Paraguay']['new_col']=10\n"
     ]
    }
   ],
   "source": [
    "#whats woring with following code->goal is to create new column and set the value for 'Paraguay' to 10\n",
    "c[c.Country=='Paraguay']['new_col']=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code handles above error i.e create col: new_col which has value: 10 for 'Country' column having value: Paraguay, rest of rows of new_col would have nulls\n",
    "c.loc[c['Country'] == 'Paraguay', 'new_col'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Paraguay</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country         Region  new_col\n",
       "189  Paraguay  SOUTH AMERICA     10.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[c.Country=='Paraguay'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benin</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Botswana</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Burkina</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country  Region  new_col\n",
       "0   Algeria  AFRICA      NaN\n",
       "1    Angola  AFRICA      NaN\n",
       "2     Benin  AFRICA      NaN\n",
       "3  Botswana  AFRICA      NaN\n",
       "4   Burkina  AFRICA      NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parllel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize each row of 2d array (list) to vary between 0 and 1\n",
    "\n",
    "make sure code can execute on multiple cpu's\n",
    "\n",
    "input:[[2, 3, 4, 5], [6, 9, 10, 12], [11, 12, 13, 14], [21, 24, 25, 26]]\n",
    "\n",
    "Output:[[0.0, 0.3333333333333333, 0.6666666666666666, 1.0], [0.0, 0.5, 0.6666666666666666, 1.0], [0.0, 0.3333333333333333, 0.6666666666666666, 1.0], [0.0, 0.6, 0.8, 1.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.3333333333333333, 0.6666666666666666, 1.0], [0.0, 0.5, 0.6666666666666666, 1.0], [0.0, 0.3333333333333333, 0.6666666666666666, 1.0], [0.0, 0.6, 0.8, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample 2D array of lists\n",
    "data = np.array([[2, 3, 4, 5], [6, 9, 10, 12], [11, 12, 13, 14], [21, 24, 25, 26]])\n",
    "\n",
    "# Min-Max scaling to normalize the data between 0 and 1\n",
    "min_vals = np.min(data, axis=1, keepdims=True)\n",
    "max_vals = np.max(data, axis=1, keepdims=True)\n",
    "\n",
    "normalized_data = (data - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "print(normalized_data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Multiple CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[2, 3, 4, 5], [6, 9, 10, 12], [11, 12, 13, 14], [21, 24, 25, 26]]\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "def normalize_row(row):\n",
    "    min_val = min(row, axis=1, keepdims=True)\n",
    "    max_val = max(row, axis=1, keepdims=True)\n",
    "    return [(val - min_val) / (max_val - min_val) for val in row]\n",
    "\n",
    "# Normalize each row of the 2D array between 0 and 1 using multiple CPUs\n",
    "def normalize_2d_array(data):\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    normalized_data = pool.map(normalize_row, data)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = normalize_2d_array(data)\n",
    "\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOPS-github actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the table structure\n",
    "\n",
    "CREATE TABLE author(\n",
    "    A_ID int NOT NULL,\n",
    "    Name varchar(100),\n",
    "    PRIMARY KEY(A_ID )\n",
    ")\n",
    "\n",
    "CREATE TABLE books(\n",
    "   B_ID int NOT NULL PRIMARY KEY,\n",
    "   Name varchar(100),\n",
    "   Price int NOT NULL,\n",
    "   A_ID int FOREIGN KEY REFERENCES author(A_ID)\n",
    "); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Program should read ddl statements and parse column names, data types and length of the columns and constraints\n",
    "\n",
    "- Program should generate required number of rows for parent and child tables  (can be taken as a parameter)\n",
    "\n",
    "- Generated data of both tables should follow the normalization rules and also foreign key constraints and data types\n",
    "\n",
    "- write approriate test cases wtih python test framework(unit test or pytest)\n",
    "\n",
    "- code should be committed to Git only when all the testcases are passd (use git actions) , this step acts as a ci/cd step.\n",
    "\n",
    "- Code should be using oops concets and pep 8 standards with proper documentation\n",
    "\n",
    "- result data should be saved in parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have attached following three files along with this jupyter notebook as part of email, Please find below files with description for each of them.\n",
    "\n",
    "1) Below Cells contains Class which takes care of Creating db, Creating table, Inserting data, fetching data using pandas, writing to Parquet format.\n",
    "2) Class file for above point 1 logic. (has same code)\n",
    "3) test_file_forclass.py - code for pytest test cases.\n",
    "3) pythonapp.yml - yaml file for github actions to check if the code is good for commit. (tested on github)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "class SQLiteParquetManager:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def create_table(self, table_name, columns):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create the table using the provided columns\n",
    "        create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def insert_data(self, table_name, data):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the table\n",
    "        insert_query = f\"INSERT INTO {table_name} VALUES ({', '.join(['?'] * len(data))})\"\n",
    "        cursor.execute(insert_query, data)\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def retrieve_data_save_parquet(self, table_name,output_file):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Retrieve all data from the table\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        data = cursor.fetchall()\n",
    "\n",
    "        \n",
    "        \n",
    "        # Assuming the data contains column names (replace with actual column names if needed)\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        df = pd.DataFrame(data, columns=column_names)\n",
    "        print(df.head(10))\n",
    "\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        pq.write_table(pa.Table.from_pandas(df), output_file)\n",
    "        if df.empty is False:\n",
    "            print('data succesfully written to parquet format')\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A_ID                 Name\n",
      "0     1          Jane Austen\n",
      "1     2         J.K. Rowling\n",
      "2     3          Leo Tolstoy\n",
      "3     4           Mark Twain\n",
      "4     5           Harper Lee\n",
      "5     6  William Shakespeare\n",
      "6     7      Agatha Christie\n",
      "7     8         Stephen King\n",
      "8     9        George Orwell\n",
      "9    10      Charles Dickens\n",
      "data succesfully written to parquet format\n"
     ]
    }
   ],
   "source": [
    "db_file = \"my_database.db\"\n",
    "manager = SQLiteParquetManager(db_file)\n",
    "\n",
    "# Create author table\n",
    "table_name = \"author\"\n",
    "columns = [\"A_ID INTEGER PRIMARY KEY\",\"Name VARCHAR(100)\"]\n",
    "manager.create_table(table_name, columns)\n",
    "\n",
    "\n",
    "authors_name =[(1, 'Jane Austen'), (2, 'J.K. Rowling'), (3, 'Leo Tolstoy'), (4, 'Mark Twain'), (5, 'Harper Lee'), (6, 'William Shakespeare'), (7, 'Agatha Christie'), (8, 'Stephen King'), (9, 'George Orwell'), (10, 'Charles Dickens')]\n",
    "\n",
    "# write data to author table\n",
    "for i in authors_name:\n",
    "    manager.insert_data(table_name, i)\n",
    "\n",
    "# Retrieve data and save as Parquet\n",
    "output_parquet_file = table_name + \".parquet\"\n",
    "retrieved_data = manager.retrieve_data_save_parquet(table_name,output_parquet_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B_ID     Name  Price  A_ID\n",
      "0   101   Book 1     25     1\n",
      "1   102   Book 2     30     2\n",
      "2   103   Book 3     20     3\n",
      "3   104   Book 4     15     1\n",
      "4   105   Book 5     28     4\n",
      "5   106   Book 6     22     5\n",
      "6   107   Book 7     35     2\n",
      "7   108   Book 8     18     6\n",
      "8   109   Book 9     40     1\n",
      "9   110  Book 10     27     7\n",
      "data succesfully written to parquet format\n"
     ]
    }
   ],
   "source": [
    "db_file = \"my_database.db\"\n",
    "manager = SQLiteParquetManager(db_file)\n",
    "\n",
    "# Create author table\n",
    "table_name = \"books\"\n",
    "columns = [\"B_ID int NOT NULL PRIMARY KEY\",\n",
    "        \"Name varchar(100)\",\n",
    "        \"Price int NOT NULL\",\n",
    "        \"A_ID int\",\n",
    "        \"FOREIGN KEY (A_ID) REFERENCES author(A_ID)\"]\n",
    "manager.create_table(table_name, columns)\n",
    "\n",
    "\n",
    "books_data = [\n",
    "    (101, 'Book 1', 25, 1),\n",
    "    (102, 'Book 2', 30, 2),\n",
    "    (103, 'Book 3', 20, 3),\n",
    "    (104, 'Book 4', 15, 1),\n",
    "    (105, 'Book 5', 28, 4),\n",
    "    (106, 'Book 6', 22, 5),\n",
    "    (107, 'Book 7', 35, 2),\n",
    "    (108, 'Book 8', 18, 6),\n",
    "    (109, 'Book 9', 40, 1),\n",
    "    (110, 'Book 10', 27, 7),\n",
    "]\n",
    "\n",
    "# write data to author table\n",
    "for i in books_data:\n",
    "    manager.insert_data(table_name, i)\n",
    "\n",
    "# Retrieve data and save as Parquet\n",
    "output_parquet_file = table_name + \".parquet\"\n",
    "retrieved_data = manager.retrieve_data_save_parquet(table_name,output_parquet_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('prophet_dask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4957677facd8a6e72df18d44ce65e114ed39d9e074aafecc080edd6ab3e46e6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
